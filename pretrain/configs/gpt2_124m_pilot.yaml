# GPT-2 124M Pilot Configuration
# 
# This config is designed for a fast pilot run on a single A40 (48GB).
# Target: ~200M tokens processed (adjustable via max_steps)
#
# Compute budget:
#   total_batch_size * max_steps = 262144 * 763 ≈ 200M tokens
#
# Usage:
#   python pretrain/train_nanogpt.py \
#     --config pretrain/configs/gpt2_124m_pilot.yaml \
#     --data-root data/fineweb_pilot/baseline \
#     --output-dir outputs/pretrain_baseline_pilot \
#     --condition baseline

model:
  # GPT-2 124M architecture
  n_layer: 12
  n_head: 12
  n_embd: 768
  block_size: 1024

training:
  # Micro batch size (per GPU)
  # A40 with 48GB should handle batch_size=16 with 1024 seq_len in bf16
  batch_size: 16
  
  # Total batch size in tokens per optimizer step
  # 262144 = 2^18 ≈ 0.25M tokens per step
  total_batch_size: 262144
  
  # Number of optimizer steps
  # 200M tokens / 262144 tokens per step ≈ 763 steps
  max_steps: 763
  
  # Warmup steps (roughly 3% of total)
  warmup_steps: 25
  
  # Learning rate schedule
  max_lr: 6.0e-4
  min_lr: 6.0e-5
  
  # Regularization
  weight_decay: 0.1
  grad_clip: 1.0
  
  # Optional: torch.compile for speedup (may have issues on some setups)
  compile: false

eval:
  # Evaluate every N steps
  interval: 50
  # Number of batches for validation loss
  val_steps: 20

logging:
  # Log every N steps
  interval: 10

checkpoint:
  # Save checkpoint every N steps
  interval: 250
  # Always save final checkpoint
  save_final: true

