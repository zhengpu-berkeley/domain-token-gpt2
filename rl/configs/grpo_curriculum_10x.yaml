# GRPO Configuration with Shaped Rewards - 10x Compute
#
# Changes from grpo_shaped.yaml:
# - 10x samples (10K vs 1K)
# - 2 epochs instead of 1
# - Designed for curriculum-trained models
#
# Usage:
#   python rl/grpo_train.py \
#     --model-path outputs/curriculum_baseline/stage_gsm8k_mixed \
#     --output-dir outputs/grpo_curriculum_baseline \
#     --condition baseline \
#     --config rl/configs/grpo_curriculum_10x.yaml

training:
  # Dataset size - 10x compute (full GSM8K train set ~7.5K)
  max_samples: 7473
  
  # Batch size per device
  batch_size: 4
  
  # Gradient accumulation (effective batch = 4 * 8 = 32)
  gradient_accumulation_steps: 8
  
  # Number of generations per prompt for GRPO
  num_generations: 4
  
  # Max completion length
  max_completion_length: 256
  
  # Sampling temperature
  temperature: 0.7
  
  # Learning rate (lower for stability with more steps)
  learning_rate: 3.0e-7
  
  # Number of epochs (2x for 10x total compute)
  num_train_epochs: 2
  
  # KL penalty coefficient (keep strong to prevent mode collapse)
  beta: 0.5
  
  # Gradient checkpointing
  gradient_checkpointing: false
  
  # Logging
  logging_steps: 25
  save_steps: 500
  eval_steps: 500

reward:
  # Use shaped rewards
  shaped: true
  
  # Reward components
  correct_answer: 1.0      # Full credit for exact match
  partial_answer: 0.3      # Partial credit for close answer
  has_steps: 0.1           # Credit for showing work (has #### or =)
  uses_mul_tokens: 0.2     # Bonus for using mul-tokens (mul_tokens condition)
  short_response: 0.1      # Bonus for not hitting max length
  
  # Penalty for degenerate behavior
  repetition_penalty: -0.3  # Penalty for repetitive patterns

