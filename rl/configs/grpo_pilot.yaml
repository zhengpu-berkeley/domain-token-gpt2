# GRPO Pilot Configuration
#
# Group Relative Policy Optimization on GSM8K for the pilot experiment.
# Compute-matched across conditions: same hyperparameters for both.
# Optimized for RTX 4090 (24GB VRAM).
#
# Compatible with TRL 0.26.2+.
#
# Usage:
#   python rl/grpo_train.py \
#     --model-path outputs/sft_gsm8k_baseline \
#     --output-dir outputs/grpo_baseline \
#     --condition baseline \
#     --config rl/configs/grpo_pilot.yaml

training:
  # Number of samples to use (null = all ~7.5k GSM8K train samples)
  max_samples: 1000
  
  # Batch size per device (conservative for 24GB VRAM)
  batch_size: 2
  
  # Gradient accumulation (effective batch = 2 * 8 = 16 prompts)
  gradient_accumulation_steps: 8
  
  # Number of generations per prompt for GRPO
  # With batch_size=2 and num_generations=4, we generate 8 completions per step
  num_generations: 4
  
  # Generation parameters (TRL 0.26.2: max_completion_length, not max_new_tokens)
  max_completion_length: 256
  temperature: 0.7
  
  # Learning rate (very small for RL fine-tuning)
  learning_rate: 1.0e-6
  
  # Training epochs
  num_train_epochs: 1
  
  # KL penalty coefficient (TRL 0.26.2: beta, not kl_coef)
  # Set to 0.0 to disable KL penalty (per recent research recommendations)
  # Set to 0.1 for moderate KL constraint
  beta: 0.1
  
  # Memory optimization
  gradient_checkpointing: true
  
  # Logging frequency
  logging_steps: 10
  
  # Checkpoint saving
  save_steps: 100
