# GRPO Pilot Configuration
#
# Group Relative Policy Optimization on GSM8K for the pilot experiment.
# Compute-matched across conditions: same hyperparameters for both.
#
# Usage:
#   python rl/grpo_train.py \
#     --model-path outputs/sft_baseline_pilot \
#     --output-dir outputs/grpo_baseline_pilot \
#     --condition baseline \
#     --config rl/configs/grpo_pilot.yaml

training:
  # Number of samples to use (null = all)
  max_samples: 1000
  
  # Batch size per device
  batch_size: 2
  
  # Gradient accumulation
  gradient_accumulation_steps: 8
  
  # Number of generations per prompt for GRPO
  num_generations: 4
  
  # Generation parameters
  max_new_tokens: 256
  temperature: 0.7
  
  # Learning rate (very small for RL)
  learning_rate: 1.0e-6
  
  # Training epochs
  num_train_epochs: 1
  
  # KL penalty coefficient
  kl_coef: 0.1

