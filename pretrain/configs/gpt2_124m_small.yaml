# GPT-2 124M Small Pilot Configuration
# 
# Smaller config for quick testing with ~50M tokens.
#
# Compute budget:
#   total_batch_size * max_steps = 262144 * 190 â‰ˆ 50M tokens
#
# Usage:
#   python pretrain/train_nanogpt.py \
#     --config pretrain/configs/gpt2_124m_small.yaml \
#     --data-root data/fineweb_pilot/baseline \
#     --output-dir outputs/pretrain_baseline_pilot \
#     --condition baseline

model:
  # GPT-2 124M architecture
  n_layer: 12
  n_head: 12
  n_embd: 768
  block_size: 1024

training:
  # Micro batch size (per GPU)
  batch_size: 16
  
  # Total batch size in tokens per optimizer step
  total_batch_size: 262144
  
  # Number of optimizer steps for ~50M tokens
  max_steps: 190
  
  # Warmup steps
  warmup_steps: 10
  
  # Learning rate schedule
  max_lr: 6.0e-4
  min_lr: 6.0e-5
  
  # Regularization
  weight_decay: 0.1
  grad_clip: 1.0
  
  compile: false

eval:
  interval: 25
  val_steps: 10

logging:
  interval: 10

checkpoint:
  interval: 100
  save_final: true

